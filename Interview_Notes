
Q. Why host your own LLM vs use OPen AI models.
Q. FasiAPI for exposing model endpoints. Why not Flask?
Q. What is the load on API, user requests per month?
Q. What is the confluence API limit(50-100 tokens at once)?
Q. How did you take care of diagrams, tables in the confluence page?
Q. Will the confluence API be hit everytime a user asks a question or there is fixed refresh rate?
Q. What all prompt engineering was done?
Q. How does the response of the llm model look like. Will it give Diagram and page links?
Q. Which LLM model you chose and why?
Q. How did you take care of section references and linkings in the doc?
Q. Which embedding technique was used?
Q. Which vector database was used?
Q. How did you evaluate the performance of the model.
Q. MLOps findings for the model.
Q. Hosting on AWS.
